{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context='talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # 输入和输出的embedding\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        # Decoder 后的 Linear + softmax\n",
    "        self.generator = generator\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src_embed = self.src_embed(src)\n",
    "        return self.encoder(src_embed, src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        tgt_embed = self.tgt_embed(tgt)\n",
    "        return self.decoder(tgt_embed, memory, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        encode_output = self.encode(src, src_mask)\n",
    "        return self.decode(encode_output, src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        根据Decoder的隐藏状态输出一个词\n",
    "        Args:\n",
    "        d_model: Decoder输出的大小\n",
    "        vocab: 词典大小\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_output = self.proj(x)\n",
    "        return F.log_softmax(linear_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        # 加一个LayerNorm\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm + sublayer(Self-Attention / Dense) + dropout + 残差连接\n",
    "    LayerNorm放在前面，与原始论文存在差别（原始论文放在最后）\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        norm_output = self.norm(x)\n",
    "        sublayer_output = sublayer(norm_output)\n",
    "        return x + self.dropout(sublayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_word\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # self attn: q, k, v\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))  # cross attn: q, k, v\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype(np.uint8)\n",
    "    return torch.from_numpy(mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "result = subsequent_mask(6)[0]\n",
    "plt.imshow(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, droupout=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.sofrmax(scores, dim=-1)\n",
    "    if droupout is not None:\n",
    "        p_attn = droupout(p_attn)\n",
    "    \n",
    "    return torch.matmul(p_attn, v), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamavid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
